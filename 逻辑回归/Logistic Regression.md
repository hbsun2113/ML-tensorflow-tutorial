##模型训练
1。线性回归的损失函数是平方损失。逻辑回归的损失函数是对数损失函数

2.正则化在逻辑回归建模中极其重要。如果没有正则化，逻辑回归的渐近性会不断促使损失在高维度空间内达到 0。因此，大多数逻辑回归模型会使用以下两个策略之一来降低模型复杂性：

L2 正则化。
早停法，即，限制训练步数或学习速率。
（我们会在之后的单元中讨论第三个策略 - L1 正则化。）

假设您向每个样本分配一个唯一 ID，且将每个 ID 映射到其自己的特征。如果您未指定正则化函数，模型会变得完全过拟合。这是因为模型会尝试促使所有样本的损失达到 0 但始终达不到，从而使每个指示器特征的权重接近正无穷或负无穷。当有大量罕见的特征组合且每个样本中仅一个时，包含特征组合的高维度数据会出现这种情况。

幸运的是，使用 L2 或早停法可以防止出现此类问题。

##二元分类 (binary classification)

一种分类任务，可输出两种互斥类别之一。例如，对电子邮件进行评估并输出“垃圾邮件”或“非垃圾邮件”的机器学习模型就是一个二元分类器。

##ROC和曲线下面积
降低分类阈值会导致将更多样本归为正类别，从而增加假正例和真正例的个数。所以ROC曲线是上扬的。

曲线下面积的一种解读方式是看作模型将某个随机正类别样本排列在某个随机负类别样本之上的概率。



